#!/bin/bash
#
# Main controller. Final, correct version with explicit I/O and enhanced metadata.
#
set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# --- Configuration & Defaults ---
ZFS_FD_BASE_DIR="/var/lib/zfs-fd"
ZFS_FD_CLONE_BASE="deep_chll/tmp/zfs-fd-analysis"
ZFS_FD_SOURCE="deep_chll/backup"
LOCK_FILE="/var/lock/zfs-fd.lock"

# --- Global Variables ---
tmp_dir=""

# --- Logging ---
log() {
    echo "[$(date -Iseconds)] [zfs-fd] $1" >&2
}

# --- Help Function ---
show_help() {
    cat << HELP
zfs-fd: File Distribution Analysis Tool

Usage: zfs-fd run [source-fs] [--run-id <id>]

Commands:
  run            Run a complete file distribution analysis
  help           Show this help message

Arguments:
  source-fs      ZFS filesystem to analyze (default: deep_chll/backup)
  --run-id <id>  Tag this analysis with an ID (optional)

Example:
  zfs-fd run deep_chll/backup --run-id backup-job-123

Exit Codes:
  0  Success
  1  Error
HELP
}

# --- Cleanup Trap ---
cleanup_on_exit() {
    local exit_code=$?
    if [ -f "$LOCK_FILE" ] && [ "$(cat $LOCK_FILE 2>/dev/null)" == "$$" ]; then
        log "EXIT trap: Releasing lock file."
        sudo rm -f "$LOCK_FILE"
    fi
    if [ -n "$tmp_dir" ] && [ -d "$tmp_dir" ]; then
        log "EXIT trap: Removing temporary directory: $tmp_dir"
        rm -rf "$tmp_dir"
    fi
    if [ $exit_code -ne 0 ]; then
        log "âš ï¸  Workflow failed unexpectedly. See log for details."
    fi
}

# --- Main Workflow ---
run_complete_workflow() {
    trap cleanup_on_exit EXIT HUP INT QUIT PIPE TERM

    # --- Argument Parsing ---
    local source_fs=""
    local run_id=""
    while [[ $# -gt 0 ]]; do
        case $1 in
            --run-id) if [ -n "$2" ]; then run_id="$2"; shift; shift; else log "Error: --run-id needs an argument"; exit 1; fi ;;
            *) if [ -z "$source_fs" ]; then source_fs="$1"; shift; else log "Error: Unknown argument $1"; exit 1; fi ;;
        esac
    done
    source_fs=${source_fs:-$ZFS_FD_SOURCE}

    # --- Logging and Lock Setup ---
    local results_dir="$ZFS_FD_BASE_DIR/$(date +%Y-%m-%dT%H-%M-%S%z)"
    sudo mkdir -p "$results_dir"
    local log_file="$results_dir/zfs-fd-run.log"
    local metadata_file="$results_dir/metadata.json"

    # --- Setup Logging and Output Redirection ---
    exec 3>&1 1>>"$log_file" 2> >(tee -a "$log_file" >&2)

    # --- Initialize Metadata ---
    echo '{"run":{},"paths":{},"timing":{}}' | sudo tee "$metadata_file" > /dev/null
    sudo jq --arg val "$source_fs" '.run.source_fs = $val' "$metadata_file" | sudo tee "$metadata_file.tmp" >/dev/null && sudo mv "$metadata_file.tmp" "$metadata_file"
    if [ -n "$run_id" ]; then
      sudo jq --arg val "$run_id" '.run.run_id = $val' "$metadata_file" | sudo tee "$metadata_file.tmp" >/dev/null && sudo mv "$metadata_file.tmp" "$metadata_file"
    fi
    sudo jq --arg val "$results_dir" '.paths.results_dir = $val' "$metadata_file" | sudo tee "$metadata_file.tmp" >/dev/null && sudo mv "$metadata_file.tmp" "$metadata_file"
    sudo jq --arg val "$log_file" '.paths.log_file = $val' "$metadata_file" | sudo tee "$metadata_file.tmp" >/dev/null && sudo mv "$metadata_file.tmp" "$metadata_file"
    sudo jq --arg val "$results_dir/filelist.txt" '.paths.filelist = $val' "$metadata_file" | sudo tee "$metadata_file.tmp" >/dev/null && sudo mv "$metadata_file.tmp" "$metadata_file"
    sudo jq --arg val "$results_dir/analysis.json" '.paths.analysis = $val' "$metadata_file" | sudo tee "$metadata_file.tmp" >/dev/null && sudo mv "$metadata_file.tmp" "$metadata_file"

    log "ðŸš€ Starting workflow. Results dir: $results_dir"
    sudo jq --arg val "$(date -Iseconds)" '.timing.run_start = $val' "$metadata_file" | sudo tee "$metadata_file.tmp" >/dev/null && sudo mv "$metadata_file.tmp" "$metadata_file"

    if [[ $EUID -ne 0 ]]; then log "âŒ Error: Must be run with sudo"; exit 1; fi
    log "Acquiring lock: $LOCK_FILE"
    if [ -f "$LOCK_FILE" ]; then log "âŒ Error: Lock file found."; exit 1; fi
    echo $$ | sudo tee "$LOCK_FILE" > /dev/null
    log "âœ… Lock acquired."

    tmp_dir=$(mktemp -d)
    log "Working in temp dir: $tmp_dir"

    log "Pre-flight cleanup..."
    "$SCRIPT_DIR/zfs-fd-cleanup" --clone-base "$ZFS_FD_CLONE_BASE" --yes
    log "âœ… System state is clean."

    # --- ZFS Phase ---
    local snapshot_tag="du-holder-$(date +%s)"
    local full_snapshot_name="$source_fs@$snapshot_tag"

    log "ðŸ“¸ Step 1/5: Creating snapshot..."
    "$SCRIPT_DIR/zfs-fd-snapshot" --source "$source_fs" --clone-base "$ZFS_FD_CLONE_BASE" --snapshot-tag "$snapshot_tag"
    
    log "ðŸ”Œ Step 2/5: Mounting clones..."
    local mount_base=$("$SCRIPT_DIR/zfs-fd-mount" --clone-base "$ZFS_FD_CLONE_BASE")
    if [ -z "$mount_base" ]; then
        log "âŒ Error: Mount failed. Proceeding to cleanup."
        "$SCRIPT_DIR/zfs-fd-cleanup" --clone-base "$ZFS_FD_CLONE_BASE" --snapshot-tag "$full_snapshot_name" --yes
        exit 1
    fi
    log "âœ… Mount point: $mount_base"

    log "ðŸ“Š Step 3/5: Capturing file metadata..."
    sudo jq --arg val "$(date -Iseconds)" '.timing.capture_start = $val' "$metadata_file" | sudo tee "$metadata_file.tmp" >/dev/null && sudo mv "$metadata_file.tmp" "$metadata_file"
    local filelist_path="$tmp_dir/filelist.txt"
    "$SCRIPT_DIR/zfs-fd-capture" --mount-base "$mount_base" --output "$filelist_path"
    sudo mv "$filelist_path" "$results_dir/filelist.txt"
    sudo jq --arg val "$(date -Iseconds)" '.timing.capture_end = $val' "$metadata_file" | sudo tee "$metadata_file.tmp" >/dev/null && sudo mv "$metadata_file.tmp" "$metadata_file"
    log "âœ… Capture complete."

    log "ðŸ§¹ Step 4/5: Cleaning up ZFS resources..."
    "$SCRIPT_DIR/zfs-fd-cleanup" --clone-base "$ZFS_FD_CLONE_BASE" --snapshot-tag "$full_snapshot_name" --yes
    log "âœ… ZFS resources are clean."

    sudo rm -f "$LOCK_FILE"
    log "âœ… Lock released."

    # --- Offline Processing ---
    log "ðŸ” Step 5/5: Processing captured data..."
    sudo jq --arg val "$(date -Iseconds)" '.timing.process_start = $val' "$metadata_file" | sudo tee "$metadata_file.tmp" >/dev/null && sudo mv "$metadata_file.tmp" "$metadata_file"
    local awk_output_path="$tmp_dir/awk_output.txt"
    "$SCRIPT_DIR/zfs-fd-process" --input "$results_dir/filelist.txt" --base-path "$mount_base" > "$awk_output_path"

    # Start compression in background immediately after processing
    xz -3 "$results_dir/filelist.txt" &
    COMPRESS_PID=$!

    local final_json_path="$results_dir/analysis.json"
    "$SCRIPT_DIR/zfs-fd-postprocess" --input "$awk_output_path" --output "$final_json_path"

    # Wait for compression and verify success
    if wait $COMPRESS_PID && [[ -f "$results_dir/filelist.txt.xz" ]]; then
        log "âœ… Filelist compression complete: filelist.txt.xz"
    else
        log "âš ï¸  Filelist compression failed or incomplete"
    fi
    sudo jq --arg val "$(date -Iseconds)" '.timing.process_end = $val' "$metadata_file" | sudo tee "$metadata_file.tmp" >/dev/null && sudo mv "$metadata_file.tmp" "$metadata_file"
    log "âœ… Processing complete."
    
    rm -rf "$tmp_dir"
    tmp_dir=""
    
    sudo jq --arg val "$(date -Iseconds)" '.timing.run_end = $val' "$metadata_file" | sudo tee "$metadata_file.tmp" >/dev/null && sudo mv "$metadata_file.tmp" "$metadata_file"
    log "ðŸŽ‰ Workflow finished successfully!"
    
    trap - EXIT
    
    # Restore original stdout and print the final JSON
    exec 1>&3 3>&-
    cat "$metadata_file"
}

# --- Main Dispatcher ---
COMMAND="${1:-help}"
case "$COMMAND" in
  help|--help|-h|"")
    show_help ;;
  run)
    shift; run_complete_workflow "$@" ;;
  *)
    if [ -f "$SCRIPT_DIR/zfs-fd-$COMMAND" ]; then
        exec "$SCRIPT_DIR/zfs-fd-$COMMAND" "$@"
    else
        log "Unknown command: $COMMAND"
        exit 1
    fi ;;
esac

# DONE.
