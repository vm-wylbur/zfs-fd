#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.13"
# dependencies = [
#     "typer",
#     "loguru",
# ]
# ///

# Author: PB and Claude
# Date: 2025-07-18
# License: (c) HRDAG, 2025, GPL-2 or newer
#
# ------
# zfs-fd/zfs-fd-process

"""
ZFS-FD filelist processor: Convert raw file metadata into directory size analysis.

Processes output from zfs-fd-capture (filelist.txt) into structured directory
size analysis with configurable depth aggregation.
"""

import json
import sys
import os
from pathlib import Path
from dataclasses import dataclass, field
from collections import defaultdict
import heapq

import typer
from loguru import logger


@dataclass
class FileEntry:
    """Represents a file with path and size."""
    path: str
    size: int
    
    def __lt__(self, other):
        """Enable comparison for heapq operations."""
        return self.size < other.size


@dataclass  
class DirectoryStats:
    """Statistics for a directory."""
    total_size: int = 0
    file_count: int = 0
    largest_files: list[FileEntry] = field(default_factory=list)
    
    def add_file(self, file_entry: FileEntry) -> None:
        """Add a file to this directory's stats."""
        self.total_size += file_entry.size
        self.file_count += 1
        
        # Track largest files (top 10) using min-heap
        if len(self.largest_files) < 10:
            heapq.heappush(self.largest_files, file_entry)
        elif file_entry.size > self.largest_files[0].size:
            heapq.heappushpop(self.largest_files, file_entry)
    
    def to_dict(self) -> dict:
        """Convert to dictionary format for JSON output."""
        # Sort largest files by size descending
        sorted_files = [
            {"path": entry.path, "size": entry.size}
            for entry in sorted(self.largest_files, key=lambda x: x.size, reverse=True)
        ]
        
        return {
            "total_size": self.total_size,
            "file_count": self.file_count, 
            "largest_files": sorted_files
        }


def process_filelist(input_path: str, base_path: str, depth: int = 3) -> dict:
    """
    Process filelist.txt into directory size analysis.
    
    Args:
        input_path: Path to filelist.txt (format: "size /full/path")
        base_path: Base path to strip from file paths  
        depth: Directory depth for aggregation
        
    Returns:
        Dict with 'summary' and 'directories' keys
    """
    logger.info(f"Processing {input_path} with base_path={base_path}, depth={depth}")
    
    base_path_obj = Path(base_path)
    directories = defaultdict(DirectoryStats)
    total_bytes = 0
    total_files = 0
    
    with open(input_path, 'r', encoding='utf-8', errors='replace') as f:
        for line_num, line in enumerate(f, 1):
            if line_num % 100_000 == 0:
                logger.debug(f"Processed {line_num:,} lines")
                
            line = line.strip()
            if not line:
                continue
                
            try:
                # Parse: "filesize /full/path"  
                parts = line.split(' ', 1)
                if len(parts) != 2:
                    logger.warning(f"Malformed line {line_num}: {line}")
                    continue
                    
                size_str, full_path = parts
                size = int(size_str)
                
                # Convert to relative path using pathlib
                full_path_obj = Path(full_path)
                try:
                    relative_path = full_path_obj.relative_to(base_path_obj)
                except ValueError:
                    logger.warning(f"Path not under base_path at line {line_num}: {full_path}")
                    continue
                
                # Truncate to depth for aggregation key
                if len(relative_path.parts) > depth:
                    dir_key = str(Path(*relative_path.parts[:depth]))
                else:
                    # Handle files at or above the depth limit
                    if relative_path.parent != Path('.'):
                        dir_key = str(relative_path.parent)
                    else:
                        dir_key = str(relative_path.parts[0]) if relative_path.parts else '.'
                
                # Create file entry and add to directory stats
                file_entry = FileEntry(path=str(relative_path), size=size)
                directories[dir_key].add_file(file_entry)
                
                total_bytes += size
                total_files += 1
                    
            except (ValueError, IndexError) as e:
                logger.warning(f"Error processing line {line_num}: {e}")
                continue
    
    logger.info(f"Processed {total_files:,} files into {len(directories):,} directories")
    
    return {
        'summary': {
            'total_directories': len(directories),
            'total_bytes': total_bytes,
            'total_files': total_files
        },
        'directories': {k: v.to_dict() for k, v in directories.items()}
    }


def configure_logging(verbose: bool, progress: bool, logfile: str = None) -> None:
    """Configure loguru to log to console and optionally to file."""
    logger.remove()  # Remove default handler
    
    # Console logging
    console_level = "DEBUG" if verbose else ("INFO" if progress else "WARNING")
    logger.add(sys.stderr, level=console_level, format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | {message}")
    
    # File logging if specified
    if logfile:
        # Ensure log directory exists
        log_path = Path(logfile)
        log_path.parent.mkdir(parents=True, exist_ok=True)
        
        logger.add(
            logfile, 
            level="DEBUG", 
            format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{function}:{line} - {message}",
            mode="a"  # Append to existing log file
        )
        logger.debug(f"Logging to file: {logfile}")


def main(
    input: str = typer.Option(..., help="Input filelist.txt path"),
    output: str = typer.Option(..., help="Output JSON file path"), 
    base_path: str = typer.Option(..., help="Base path to strip from file paths"),
    depth: int = typer.Option(3, help="Directory depth for aggregation"),
    logfile: str = typer.Option(None, help="Optional log file path"),
    progress: bool = typer.Option(False, help="Show progress indicator"),
    verbose: bool = typer.Option(False, help="Enable verbose logging")
) -> None:
    """Process ZFS-FD filelist into directory size analysis."""
    
    configure_logging(verbose, progress, logfile)
    logger.info("Starting zfs-fd-process")
    
    try:
        result = process_filelist(input, base_path, depth)
        
        with open(output, 'w') as f:
            json.dump(result, f, indent=2)
            
        logger.success(f"Output written to {output}")
        if progress:
            summary = result['summary']
            logger.info(f"Summary: {summary['total_files']:,} files, {summary['total_directories']:,} directories, {summary['total_bytes']:,} bytes")
            
    except FileNotFoundError as e:
        logger.error(f"File not found: {e}")
        raise typer.Exit(1)
    except Exception as e:
        logger.error(f"Processing error: {e}")
        raise typer.Exit(1)
    finally:
        logger.info("zfs-fd-process completed")


if __name__ == "__main__":
    typer.run(main)
