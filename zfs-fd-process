#!/bin/bash
# Author: PB and Claude
# Date: 2025-08-08
# License: (c) HRDAG, 2025, GPL-2 or newer
#
# ------
# zfs-fd/zfs-fd-process
#
# Process TSV file from zfs-fd-capture and aggregate by directory

set -euo pipefail

# --- Argument Parsing ---
INPUT_FILE=""
BASE_PATH=""

while [[ $# -gt 0 ]]; do
  case $1 in
    --input)
      INPUT_FILE="$2"
      shift; shift
      ;;
    --base-path)
      BASE_PATH="$2"
      shift; shift
      ;;
    *)
      echo "Usage: $0 --input <file> --base-path <path>" >&2
      exit 1
      ;;
  esac
done

if [ ! -f "$INPUT_FILE" ] || [ -z "$BASE_PATH" ]; then
    echo "Usage: $0 --input <file> --base-path <path>" >&2
    exit 1
fi

# --- AWK Processing ---
AWK=gawk
if ! command -v $AWK &> /dev/null; then AWK=awk; fi

# Process the TSV file (FS=0x1C field separator)
# Input format: dataset<FS>size<FS>mtime<FS>fullpath
# We need fields 2 (size) and 4 (fullpath)

$AWK -v base_path="$BASE_PATH" '
BEGIN {
    FS = "\034"  # ASCII 0x1C field separator
    OFS = "\t"
    base_path_len = length(base_path)
}

# Skip comment lines
/^#/ { next }

# Process data lines
NF >= 4 {
    size = $2
    path = $4
    dataset = $1
    
    # Extract dataset component (home, zsd, backup, etc.)
    # Dataset format: deep_chll/backup/home or deep_chll/backup/zsd or just deep_chll/backup
    dataset_prefix = ""
    if (match(dataset, /deep_chll\/backup\/(.+)/, arr)) {
        dataset_prefix = arr[1] "/"
    } else if (dataset == "deep_chll/backup") {
        dataset_prefix = "backup/"
    }
    
    # Extract meaningful path from snapshot path
    # Path format: /deep_chll/backup/home/.zfs/snapshot/SNAPSHOT_NAME/actual/path
    if (match(path, /\.zfs\/snapshot\/[^\/]+\//)) {
        # Get everything after the snapshot directory
        rel_path = substr(path, RSTART + RLENGTH)
        
        # Prepend the dataset prefix if we have one
        if (dataset_prefix != "" && dataset_prefix != "backup/") {
            full_path = dataset_prefix rel_path
        } else if (dataset_prefix == "backup/" && rel_path != "") {
            full_path = dataset_prefix rel_path
        } else {
            full_path = rel_path
        }
        
        # Split into components and aggregate up to 4 levels
        n = split(full_path, components, "/")
        
        if (n > 0) {
            # Build directory keys for different levels
            for (depth = 1; depth <= 4 && depth <= n-1; depth++) {
                dir_key = components[1]
                for (i = 2; i <= depth; i++) {
                    dir_key = dir_key "/" components[i]
                }
                dirs[dir_key] += size
                counts[dir_key] += 1
            }
        }
    }
}

END {
    # Output aggregated results
    for (dir in dirs) {
        print dirs[dir], counts[dir], dir
    }
}
' "$INPUT_FILE" | sort -k1 -rn